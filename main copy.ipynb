{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 1 \n",
    "[Kaggle](https://www.kaggle.com/competitions/2023-datalab-cup1-predicting-news-popularity/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Folder Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27643, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 14011, 1: 13632}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(df['Popularity'].values, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: <html><head><div class=\"article-info\"> <span class=\"byline basic\">Clara Moskowitz</span> for <a href=\"/publishers/space-com/\">Space.com</a> <time datetime=\"Wed, 19 Jun 2013 15:04:30 +0000\">2013-06-19 15:04:30 UTC</time> </div></head><body><h1 class=\"title\">NASA's Grand Challenge: Stop Asteroids From Destroying Earth</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\" data-micro=\"1\" data-url=\"http://mashable.com/2013/06/19/nasa-grand-challenge-asteroid/\" src=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\"/></figure><article data-channel=\"world\"><section class=\"article-content\"> <p>There may be killer asteroids headed for Earth, and NASA has decided to do something about it. The space agency announced a new \"Grand Challenge\" on June 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet.</p> <p>The new mission builds on projects already underway at NASA, including a plan to <a href=\"http://www.space.com/20591-nasa-asteroid-capture-mission-feasibility.html\" target=\"_blank\">capture an asteroid</a>, pull it in toward the moon and send astronauts to visit it. As part of the Grand Challenge, the agency issued a \"request for information\" today — aiming to solicit ideas from industry, academia and the public on how to improve the asteroid mission plan.</p> <p>\"We're asking for you to think about concepts and different approaches for what we've described here,\" William Gerstenmaier, NASA's associate administrator for human explorations and operations, said yesterday during a NASA event announcing the initiative. \"We want you to think about other ways of enhancing this to get the most out of it.\"</p> <p><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture</a></strong><br><br>Responses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due July 18.<br><br>The asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on NASA's Space Launch System rocket and Orion space capsule under development, has come under fire from lawmakers who would prefer that NASA return to the moon.<br><br>A <a href=\"http://www.space.com/21609-nasa-asteroid-capture-mission-congress.html\" target=\"_blank\">draft NASA authorization bill</a> from the House space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects. That bill will be discussed during a hearing Wednesday, June 19 at 10 a.m. EDT.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture Mission in Pictures</a></strong><br><br>But NASA officials defended the asteroid mission today and said they were confident they'd win Congress' support once they explained its benefits further.<br><br>\"I think that we really, truly are going to be able to show the value of the mission,\" NASA Associate Administrator Lori Garver said today. \"To me, this is something that what we do in this country — we debate how we spend the public's money. This is the beginning of the debate.\"<br><br>Garver also maintained that sending astronauts to an asteroid would not diminish NASA's other science and exploration goals, including another lunar landing.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20601-animation-of-proposed-asteroid-retrieval-mission-video.html\" target=\"_blank\">Animation Of Proposed Asteroid Retrieval Mission</a></strong><br><br>\"This initiative takes nothing from the other valuable work,\" she said. \"This is only a small piece of our overall strategy, but it is an integral piece. It takes nothing from the moon.\"<br><br>Part of NASA's plan to win support for the flight is to link it more closely with the larger goal of protecting Earth from asteroid threats.<br><br>If, someday, humanity discovers an asteroid headed for Earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said Tom Kalil, deputy director for technology and innovation at the White House Office of Science and Technology Policy.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20006-deep-space-missions-private-companies.html\" target=\"_blank\">Wildest Private Deep-Space Mission Ideas: A Countdown</a></strong><br><br>The topic of asteroid threats is more timely than ever, after a meteor exploded over the Russian city of <a href=\"http://www.space.com/19823-russia-meteor-explosion-complete-coverage.html\" target=\"_blank\">Chelyabinsk</a> on Feb. 15 — the same day that the football field-sized <a href=\"http://www.space.com/19646-asteroid-2012-da14-earth-flyby-complete-coverage.html\" target=\"_blank\">asteroid 2012 DA14</a> passed within the moon's orbit of Earth.<br><br><em>Image courtesy of <a href=\"http://www.dvidshub.net/image/707596/ida-and-dactyl#.UcHDQvk4uSo\" target=\"_blank\">NASA</a></em></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></p> <ul> <li><a href=\"http://www.space.com/34406-spacexs-musk-says-sabotage-unlikely-cause-of-sept-1-explosion-but-still-a-worry.html\">SpaceX's Musk Says Sabotage Unlikely Cause of Sept. 1 Explosion, But Still a Worry</a></li> <li><a href=\"http://www.space.com/34405-proxima-centauri-starspots-stellar-cycle-habitable-planet-alien-life.html\">Proxima Centauri Is Like Our Sun... on Steroids</a></li> <li><a href=\"http://www.space.com/34404-china-launches-shenzhou-11-astronauts-to-space-lab.html\">China Launches Shenzhou-11 Astronauts to Tiangong-2 Space Lab</a></li> <li><a href=\"http://www.space.com/34403-space-station-mockup-in-houston-astronaut-guided-tour-video.html\">Space Station Mockup In Houston - Astronaut Guided Tour | Video</a></li> </ul> <p> This article originally published at Space.com <a href=\"http://www.space.com/21610-nasa-asteroid-threat-grand-challenge.html?\">here</a> </p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/asteroid/\">Asteroid</a>, <a href=\"/category/asteroids/\">Asteroids</a>, <a href=\"/category/challenge/\">challenge</a>, <a href=\"/category/earth/\">Earth</a>, <a href=\"/category/space/\">Space</a>, <a href=\"/category/us/\">U.S.</a>, <a href=\"/category/world/\">World</a> </footer></body></html>\n",
      "After:  clara moskowitz for space com 2013 06 19 15 04 30 utc nasa s grand challenge stop asteroids from destroying earth there may be killer asteroids headed for earth and nasa has decided to do something about it the space agency announced a new grand challenge on june 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet the new mission builds on projects already underway at nasa including a plan to capture an asteroid pull it in toward the moon and send astronauts to visit it as part of the grand challenge the agency issued a request for information today aiming to solicit ideas from industry academia and the public on how to improve the asteroid mission plan we re asking for you to think about concepts and different approaches for what we ve described here william gerstenmaier nasa s associate administrator for human explorations and operations said yesterday during a nasa event announcing the initiative we want you to think about other ways of enhancing this to get the most out of it see also how it works nasa asteroid captureresponses to the request for information which also seeks ideas for detecting and mitigating asteroid threats are due july 18 the asteroid retrieval mission designed to provide the first deep space mission for astronauts flying on nasa s space launch system rocket and orion space capsule under development has come under fire from lawmakers who would prefer that nasa return to the moon a draft nasa authorization bill from the house space subcommittee which is currently in debate would cancel the mission and steer the agency toward other projects that bill will be discussed during a hearing wednesday june 19 at 10 a m edt see also how it works nasa asteroid capture mission in picturesbut nasa officials defended the asteroid mission today and said they were confident they d win congress support once they explained its benefits further i think that we really truly are going to be able to show the value of the mission nasa associate administrator lori garver said today to me this is something that what we do in this country we debate how we spend the public s money this is the beginning of the debate garver also maintained that sending astronauts to an asteroid would not diminish nasa s other science and exploration goals including another lunar landing see also animation of proposed asteroid retrieval mission this initiative takes nothing from the other valuable work she said this is only a small piece of our overall strategy but it is an integral piece it takes nothing from the moon part of nasa s plan to win support for the flight is to link it more closely with the larger goal of protecting earth from asteroid threats if someday humanity discovers an asteroid headed for earth and manages to alter its course it will be one of the most important accomplishments in human history said tom kalil deputy director for technology and innovation at the white house office of science and technology policy see also wildest private deep space mission ideas a countdownthe topic of asteroid threats is more timely than ever after a meteor exploded over the russian city of chelyabinsk on feb 15 the same day that the football field sized asteroid 2012 da14 passed within the moon s orbit of earth image courtesy of nasa spacex s musk says sabotage unlikely cause of sept 1 explosion but still a worry proxima centauri is like our sun on steroids china launches shenzhou 11 astronauts to tiangong 2 space lab space station mockup in houston astronaut guided tour video this article originally published at space com here topics asteroid asteroids challenge earth space u s world  \n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace ('-', '') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "print('Before:', df.iloc[0]['Page content'])\n",
    "processed_txt = preprocessor(df.iloc[0]['Page content'])\n",
    "print('After:', processed_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/out.txt\",'w') as f:\n",
    "    print(BeautifulSoup(df.iloc[0]['Page content'], 'html.parser').prettify(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/out2.txt\",'w') as f:\n",
    "    print(BeautifulSoup(df.iloc[1]['Page content'], 'html.parser').prettify(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/out3.txt\",'w') as f:\n",
    "    print(BeautifulSoup(df.iloc[300]['Page content'], 'html.parser').prettify(), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27643, 1), (27643, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['Page content'].values[:, np.newaxis]\n",
    "y = df['Popularity'].values[:, np.newaxis]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale y data from [-1, 1] to [0, 1]\n",
    "Useful for logistic regression model since y has to be [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min, max = 0, 1\n",
    "y_std = (y - y.min(axis=0)) / (y.max(axis=0) - y.min(axis=0))\n",
    "y = y_scaled = y_std * (max - min) + min\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the tags in all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Title from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_html(X):\n",
    "    titles = []\n",
    "\n",
    "    for x_data in X:\n",
    "        soup = BeautifulSoup(x_data[0], 'html.parser')\n",
    "        # print(soup.title)\n",
    "        title = soup.find_all(attrs={\"class\": \"title\"})\n",
    "        # print(title[0].get_text)\n",
    "        if title is not None:\n",
    "            titles.append(title[0].get_text())\n",
    "        else:\n",
    "            titles.append(np.nan)\n",
    "    \n",
    "    titles = np.array(titles)[:, np.newaxis]\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Category from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_html(X):\n",
    "    categories_extracted = []\n",
    "    for x_data in X:\n",
    "        categories = []\n",
    "        soup = BeautifulSoup(x_data[0], 'html.parser')\n",
    "        # print(soup.title)\n",
    "        html_cats = soup.find_all(attrs={\"href\": re.compile(r'/category/')})\n",
    "\n",
    "        for cat in html_cats:\n",
    "            # print(cat.get('href'))\n",
    "            categories.append(cat.get_text())\n",
    "\n",
    "        category_str = \"\"\n",
    "        if len(categories) != 0:\n",
    "            category_str = \" \".join(categories) \n",
    "        else:\n",
    "            category_str = np.nan\n",
    "\n",
    "        categories_extracted.append(category_str)\n",
    "        \n",
    "    categories_extracted = np.array(categories_extracted)[:, np.newaxis]\n",
    "    return categories_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Time from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_and_dates_html(X):\n",
    "    dates = []\n",
    "    days = []\n",
    "    for x_data in X:\n",
    "        soup = BeautifulSoup(x_data[0], 'html.parser')\n",
    "        # print(soup.title)\n",
    "        # find only the date when it is written\n",
    "        html_date = soup.find(\"time\")\n",
    "\n",
    "        if html_date is not None:\n",
    "            # print(cat.get('href'))\n",
    "            # ex of pattern : Wed, 19 Jun 2013 15:04:30 +0000\n",
    "            days.append(html_date.get('datetime').replace(',', ' ')[0:3])\n",
    "            dates.append(html_date.get_text())\n",
    "        else:\n",
    "            days.append(np.nan)\n",
    "            dates.append(np.nan)\n",
    "\n",
    "    return days, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date and time\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "\n",
    "def date_week_parser(day):\n",
    "    dict_days = {'Sun': 0, 'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6}\n",
    "    return dict_days[day]\n",
    "\n",
    "def date_parser(time):\n",
    "    timestamp = parser.parse(time)\n",
    "    # Also convert everything to UTC\n",
    "    timestamp = timestamp.astimezone(pytz.UTC)\n",
    "    return timestamp\n",
    "\n",
    "def get_year(dt):\n",
    "    # Extract individual components\n",
    "    return dt.year\n",
    "\n",
    "def get_month(dt):\n",
    "    return dt.month\n",
    "\n",
    "def get_date_day(dt):\n",
    "    return dt.day\n",
    "\n",
    "def get_time_second(dt):\n",
    "    hour = dt.hour\n",
    "    minute = dt.minute\n",
    "    second = dt.second\n",
    "    return hour * 3600 + minute * 60 + second\n",
    "        \n",
    "# print('before', dates[456])\n",
    "# dt = date_parser(dates[456])\n",
    "# print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute time and date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_and_time_features(X):\n",
    "    days, dates = get_days_and_dates_html(X)\n",
    "    features_day = []\n",
    "    for day in days:\n",
    "        feature_day = date_week_parser(day) \n",
    "        features_day.append(feature_day)\n",
    "\n",
    "    features_yr = []\n",
    "    features_time = []\n",
    "    features_month = []\n",
    "    features_date = []\n",
    "\n",
    "    for date in dates:\n",
    "        dt = date_parser(date)\n",
    "        yr = get_year(dt)\n",
    "        time = get_time_second(dt)\n",
    "        month = get_month(dt)\n",
    "        date_day = get_date_day(dt)\n",
    "\n",
    "        features_yr.append(yr)\n",
    "        features_time.append(time)\n",
    "        features_month.append(month)\n",
    "        features_date.append(date_day)\n",
    "\n",
    "    features_day = np.array(features_day)[:, np.newaxis]\n",
    "    features_yr = np.array(features_yr)[:, np.newaxis]\n",
    "    features_time = np.array(features_time)[:, np.newaxis]\n",
    "    features_month = np.array(features_month)[:, np.newaxis]\n",
    "    features_date = np.array(features_date)[:, np.newaxis]\n",
    "\n",
    "    return features_day, features_yr, features_time, features_month, features_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Video Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_count_html(X):\n",
    "    lst_num_videos = []\n",
    "    for x_data in X:\n",
    "        categories = []\n",
    "        soup = BeautifulSoup(x_data[0], 'html.parser')\n",
    "        # print(soup.title)\n",
    "        html_vid = soup.find_all('iframe')\n",
    "\n",
    "        num_of_videos = len(html_vid)\n",
    "        lst_num_videos.append(num_of_videos)\n",
    "        \n",
    "    lst_num_videos = np.array(lst_num_videos)[:, np.newaxis]\n",
    "    return lst_num_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Image Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_count_html(X):\n",
    "    lst_num_imgs = []\n",
    "    for x_data in X:\n",
    "        categories = []\n",
    "        soup = BeautifulSoup(x_data[0], 'html.parser')\n",
    "        # print(soup.title)\n",
    "        html_img = soup.find_all('img')\n",
    "\n",
    "        num_of_photos = len(html_img)\n",
    "        lst_num_imgs.append(num_of_photos)\n",
    "        \n",
    "    lst_num_imgs = np.array(lst_num_imgs)[:, np.newaxis]\n",
    "    return lst_num_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count_html(X):\n",
    "    lst_word_count = []\n",
    "    for x_data in X:\n",
    "        categories = []\n",
    "        soup = BeautifulSoup(x_data[0], 'html.parser')\n",
    "        # print(soup.title)\n",
    "        content_element = soup.find('section', class_='article-content')\n",
    "        content = content_element.text\n",
    "\n",
    "        word_count = len(content.split())\n",
    "        lst_word_count.append(word_count)\n",
    "        \n",
    "    lst_word_count = np.array(lst_word_count)[:, np.newaxis]\n",
    "    return lst_word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Call the function and combine data together into an np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obtain_features(X):\n",
    "    titles = get_title_html(X)\n",
    "    categories_extracted = get_categories_html(X)\n",
    "    features_day, features_yr, features_time, features_month, features_date = extract_date_and_time_features(X)\n",
    "    concatenated_features = np.concatenate((titles, categories_extracted, features_day, features_yr, features_time, features_month, features_date), axis=1)\n",
    "    return concatenated_features    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/audreych/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Feature Extracted into a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'Popularity', 'Page content']\n"
     ]
    }
   ],
   "source": [
    "old_features_name = [c for c in df.columns]\n",
    "new_features_name = ['Id', 'Popularity', 'Title', 'Category', 'Day', 'Year', 'Time', 'Month', 'Date']\n",
    "# new_features_name = [c for c in df.columns]\n",
    "print(old_features_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new column feature in the dataframe\n",
    "Given column name and also the data to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content  dummy\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...    0.0\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...    0.0\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...    0.0\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...    0.0\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...    0.0\n"
     ]
    }
   ],
   "source": [
    "def add_new_features_df(df, new_feature_name, new_data):\n",
    "    if (len(new_data.shape) == 2 and new_data.shape[1] != 1) or (len(new_data.shape) > 2):\n",
    "        print('Check dimension again, it should be 2 dimension and (sample, 1) or (sample, )')\n",
    "        return\n",
    "    elif len(new_data.shape) == 2 and new_data.shape[1] == 1:\n",
    "        new_data = new_data.reshape(-1)\n",
    "    \n",
    "    new_data = np.array(new_data)\n",
    "    if new_feature_name not in df.columns:\n",
    "        df[new_feature_name] = new_data.tolist()\n",
    "\n",
    "dummy = np.zeros((df.shape[0], 1))\n",
    "new_df = df.copy()\n",
    "add_new_features_df(new_df, 'dummy', dummy)\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function for Deleting Column in DataFrame\n",
    "delete columns given columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "def delete_features_df(df, feature_name):\n",
    "    if feature_name in df.columns:\n",
    "        df.drop(columns=feature_name, inplace=True)\n",
    "    \n",
    "delete_features_df(new_df, 'dummy')\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute and Get New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_df(X, df, new_features_name, old_features_name):\n",
    "    concatenated_features = preprocess_obtain_features(X)\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_feat_not_in_old_df = [item for item in new_features_name if item not in old_features_name]\n",
    "\n",
    "    for (feature_name, extracted_data) in zip(new_feat_not_in_old_df, np.transpose(concatenated_features)):\n",
    "        add_new_features_df(new_df, feature_name, extracted_data)\n",
    "\n",
    "    # print(new_df.head())\n",
    "    old_feat_not_in_new_df = [item for item in old_features_name if item not in new_features_name]\n",
    "\n",
    "    for feature_name in old_feat_not_in_new_df:\n",
    "        delete_features_df(new_df, feature_name)\n",
    "\n",
    "    return new_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                              Title  \\\n",
      "0   0          -1  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1   1           1  Google's New Open Source Patent Pledge: We Won...   \n",
      "2   2           1  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3   3          -1        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4   4          -1  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "\n",
      "                                            Category Day  Year   Time Month  \\\n",
      "0  Asteroid Asteroids challenge Earth Space U.S. ...   3  2013  54270     6   \n",
      "1  patent-lawsuit theater Apps and Software Googl...   4  2013  63655     3   \n",
      "2  NFL ESPN Entertainment NFL NFL Draft Sports Te...   3  2014  69320     5   \n",
      "3            YouTube Sports Video Videos Watercooler   5  2013   8810    10   \n",
      "4  NFL Instagram Entertainment instagram instagra...   4  2014  12703     4   \n",
      "\n",
      "  Date  \n",
      "0   19  \n",
      "1   28  \n",
      "2    7  \n",
      "3   11  \n",
      "4   17  \n"
     ]
    }
   ],
   "source": [
    "new_df = get_new_df(X, df, new_features_name, old_features_name)\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new features to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('./output/modif_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27643, 7), (27643, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = new_df.iloc[:, 2:]\n",
    "y = new_df.iloc[:, 1:2]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : don't leak the training data to fit the category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_fun(X_train_df, feature_name):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "    tfidf.fit(X_train_df[feature_name])\n",
    "\n",
    "    top = 10\n",
    "    # get idf score of vocabularies\n",
    "    idf = tfidf.idf_\n",
    "    print('[vocabularies with smallest idf scores]')\n",
    "    sorted_idx = idf.argsort()\n",
    "\n",
    "    for i in range(top):\n",
    "        print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "    doc_tfidf = tfidf.transform(X_train_df[feature_name]).toarray()\n",
    "    tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "    print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "    for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                            np.sort(tfidf_sum)[::-1][:top]):\n",
    "        print('{}: {}'.format(tok, v))\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize_features(X, feature_name, tfidf=None, training=False):\n",
    "    if training == True:\n",
    "        tfidf = tfidf_fun(X, feature_name)\n",
    "    else:\n",
    "        if tfidf == None:\n",
    "            print(\"Pass an existing tfidf for validation\")\n",
    "            return\n",
    "    \n",
    "    tfidf_doc = tfidf.transform(X[feature_name]).toarray()\n",
    "    # tfidf_doc_val = tfidf.transform(X_val['Category']).toarray()\n",
    "    return tfidf, tfidf_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA to reduce dimension \n",
    "This is to make dimension smaller, since we still need to tf-idf title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # scikit-learn 1.3.1\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def std_scale(X, sc=None, training=False):\n",
    "    if training == True:\n",
    "        sc = StandardScaler()\n",
    "        sc.fit(X)\n",
    "    else:\n",
    "        if sc == None:\n",
    "            print(\"Pass an existing scaler for validation\")\n",
    "            return\n",
    "\n",
    "    Z = sc.transform(X)\n",
    "    return sc, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_fun(X, n_components=3000, pca=None, training=False):\n",
    "    if training == True:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "    else:\n",
    "        if pca == None:\n",
    "            print(\"Pass an existing tfidf for validation\")\n",
    "            return\n",
    "\n",
    "    Z_pca = pca.transform(X)\n",
    "    return pca, Z_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Compressed Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pkl\n",
    "def save_trainable(sc, tfidf, pca, name='cat'):\n",
    "    # dump to disk\n",
    "    pkl.dump(sc, open(f'./output/sc_{name}.pkl', 'wb'))\n",
    "    pkl.dump(tfidf, open(f'./output/tfidf_{name}.pkl', 'wb'))\n",
    "    pkl.dump(pca, open(f'./output/pca_{name}.pkl', 'wb'))\n",
    "    # pkl.dump(Z_pca, open('./output/doc_tfidf_pca.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap it as one func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vectorize_procedure(X, feature_name, n_components_pca=3000, tfidf=None, sc=None, pca=None, training=False, file_name='cat'):\n",
    "    tfidf, tfidf_doc = tfidf_vectorize_features(X, feature_name, tfidf=tfidf, training=training)\n",
    "    sc, Z = std_scale(tfidf_doc, sc=sc, training=training)\n",
    "    pca, Z_pca = PCA_fun(Z, n_components=n_components_pca, pca=pca, training=training)\n",
    "    if training==True:\n",
    "        save_trainable(sc, tfidf, pca, name=file_name)\n",
    "    return Z_pca, tfidf, sc, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(X, Z_pca_ttl, Z_pca_cat):\n",
    "    delete_features_df(X, 'Title')\n",
    "    delete_features_df(X, 'Category')\n",
    "    X = np.concatenate((X, Z_pca_ttl, Z_pca_cat), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18428, 7) (9215, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "world: 2.37\n",
      "tech: 2.60\n",
      "entertain: 2.69\n",
      "watercool: 2.70\n",
      "busi: 2.79\n",
      "u.s.: 2.86\n",
      "video: 2.95\n",
      "app: 3.02\n",
      "softwar: 3.08\n",
      "mobil: 3.10\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "video: 985.4462807853151\n",
      "world: 788.013689491839\n",
      "busi: 633.662226007963\n",
      "watercool: 549.9942307441912\n",
      "entertain: 547.3241856611837\n",
      "tech: 544.4960612322128\n",
      "app: 532.1345287463241\n",
      "u.s.: 517.1376614649233\n",
      "twitter: 420.4982802191957\n",
      "media: 409.2324875723526\n",
      "[vocabularies with smallest idf scores]\n",
      "new: 3.94\n",
      "app: 4.48\n",
      "googl: 4.49\n",
      "video: 4.52\n",
      "twitter: 4.68\n",
      "facebook: 4.69\n",
      "get: 4.70\n",
      "make: 4.80\n",
      "appl: 4.81\n",
      "first: 4.84\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "new: 212.39890196162423\n",
      "app: 160.8213563402999\n",
      "googl: 153.4905448181874\n",
      "video: 140.55324470216217\n",
      "facebook: 133.91177288109571\n",
      "twitter: 131.24350907912617\n",
      "get: 121.81682108547119\n",
      "appl: 114.35372533748621\n",
      "make: 111.84357356467666\n",
      "know: 108.32842201464089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# depending on the kfold\n",
    "roc_auc_train = []\n",
    "roc_auc_test = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    X_train = X.iloc[train_index, :]\n",
    "    y_train = y.iloc[train_index, :]\n",
    "    X_val = X.iloc[val_index, :]\n",
    "    y_val = y.iloc[val_index, :]\n",
    "    print(X_train.shape, X_val.shape)\n",
    "    # Do this for both title and category\n",
    "    Z_pca_cat, tfidf, sc, pca = feature_vectorize_procedure(X_train, 'Category', n_components_pca=3000, tfidf=None, sc=None, pca=None, training=True, file_name='cat')\n",
    "    Z_pca_cat_val, _, _, _ = feature_vectorize_procedure(X_val, 'Category', n_components_pca=3000, tfidf=tfidf, sc=sc, pca=pca, training=False, file_name='cat')\n",
    "\n",
    "    Z_pca_ttl, tfidf, sc, pca = feature_vectorize_procedure(X_train, 'Title', n_components_pca=5000, tfidf=None, sc=None, pca=None, training=True, file_name='title')\n",
    "    Z_pca_ttl_val, _, _, _ = feature_vectorize_procedure(X_val, 'Title', n_components_pca=3000, tfidf=tfidf, sc=sc, pca=pca, training=False, file_name='title')\n",
    "\n",
    "    X_train = combine_features(X_train, Z_pca_ttl, Z_pca_cat)\n",
    "    X_val = combine_features(X_val, Z_pca_ttl_val, Z_pca_cat_val)\n",
    "\n",
    "    \n",
    "    clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    pkl.dump(clf, open(f'./output/model-k{i}.pkl', 'wb'))\n",
    "\n",
    "    roc_auc_train.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    roc_auc_test.append(roc_auc_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "\n",
    "print('avg roc_auc_train =', np.average(roc_auc_train))\n",
    "print('avg roc_auc_test =', np.average(roc_auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_pca_cat, tfidf, sc, pca = feature_vectorize_procedure(X_train, 'Category', n_components_pca=3000, tfidf=None, sc=None, pca=None, training=True, file_name='cat')\n",
    "Z_pca_ttl, tfidf, sc, pca = feature_vectorize_procedure(X_train, 'Title', n_components_pca=5000, tfidf=None, sc=None, pca=None, training=True, file_name='title')\n",
    "X_train = combine_features(X, Z_pca_ttl, Z_pca_cat)\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "pkl.dump(clf, open(f'./output/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (used to check which param is good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5374442443406697, 0.5433238487634284, 0.5433644221305467, 0.5432824901053337, 0.5461816535086801, 0.5459289206960819, 0.5433769867861705, 0.5523324450819844, 0.5549526993068499]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import _pickle as pkl\n",
    "\n",
    "# roc_auc_train = []\n",
    "# roc_auc_test = []\n",
    "\n",
    "# max_depth = 10\n",
    "# for d in range(1, max_depth):\n",
    "#     clf = RandomForestClassifier(max_depth=d, random_state=0)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     # dump to disk\n",
    "#     pkl.dump(clf, open(f'./output/model_forest_{d}.pkl', 'wb'))\n",
    "    \n",
    "#     roc_auc_train.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "#     roc_auc_test.append(roc_auc_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "    \n",
    "# plt.plot(range(1, max_depth), roc_auc_train, label = 'Training error', color = 'blue', linewidth = 2)\n",
    "# plt.plot(range(1, max_depth), roc_auc_test, label = 'Testing error', color = 'red', linewidth = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/test.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df['Page content'].values[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Python trainable obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category\n",
    "sc_cat = pkl.load(open('./output/sc_cat.pkl', 'rb'))\n",
    "tfidf_cat = pkl.load(open('./output/tfidf_cat.pkl', 'rb'))\n",
    "pca_cat = pkl.load(open('./output/pca_cat.pkl', 'rb'))\n",
    "\n",
    "# title\n",
    "sc_title = pkl.load(open('./output/sc_title.pkl', 'rb'))\n",
    "tfidf_title = pkl.load(open('./output/tfidf_title.pkl', 'rb'))\n",
    "pca_title = pkl.load(open('./output/pca_title.pkl', 'rb'))\n",
    "\n",
    "# model\n",
    "clf = pkl.load(open('./output/model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_features_name = [c for c in df.columns]\n",
    "new_features_name = ['Id', 'Title', 'Category', 'Day', 'Year', 'Time', 'Month', 'Date']\n",
    "# new_features_name = [c for c in df.columns]\n",
    "print(old_features_name)\n",
    "new_df = get_new_df(X, df, new_features_name, old_features_name)\n",
    "print(new_df.head())\n",
    "# without id\n",
    "X_test = new_df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_pca_cat, tfidf, sc, pca = feature_vectorize_procedure(X_test, 'Category', n_components_pca=3000, tfidf=None, sc=None, pca=None, training=True, file_name='cat')\n",
    "Z_pca_ttl, tfidf, sc, pca = feature_vectorize_procedure(X_test, 'Title', n_components_pca=5000, tfidf=None, sc=None, pca=None, training=True, file_name='title')\n",
    "X_test = combine_features(X_test, Z_pca_ttl, Z_pca_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "df = pd.DataFrame({'Id': df['Id'], 'Popularity': y_pred})\n",
    "df.to_csv('./output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_compe1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
