{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"f678fa19be8944b7a63090d8b184ea41","deepnote_cell_type":"markdown"},"source":["# DataLab Cup 1 \n","[Kaggle](https://www.kaggle.com/competitions/2023-datalab-cup1-predicting-news-popularity/data)"]},{"cell_type":"markdown","metadata":{"cell_id":"cb9b04cfa43c47bea79a0875796ffba9","deepnote_cell_type":"markdown"},"source":["## DL_comp1_Shallow_Learning_report\n","### Members\n","- **109000162 黃志偉**\n","- **109006243 姚林飛**\n","- **109006273 鍾宛里**"]},{"cell_type":"markdown","metadata":{"cell_id":"b8608170d9f94683b4b82a951fa37af9","deepnote_cell_type":"markdown"},"source":["- Import necessary libraries and the news' datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2181433dcf0e47abbb05a05cf7cff95a","deepnote_cell_type":"code"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"58797de846064283aeeb6667b0df4e12","deepnote_cell_type":"code"},"outputs":[],"source":["# load csv file\n","train = pd.read_csv('data/train.csv')\n","test = pd.read_csv('data/test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7f7575adb9d04a76bff9909e87155f57","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["train.shape=(27643, 3)\n","train.columns=Index(['Id', 'Popularity', 'Page content'], dtype='object')\n"]}],"source":["print(f\"{train.shape=}\")\n","print(f\"{train.columns=}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"7aa2fe8449bf4d14af906ba7a4e9dce9","deepnote_cell_type":"markdown"},"source":["### Preprocessing"]},{"cell_type":"markdown","metadata":{"cell_id":"b8b80e6b63494e9c9d752bf9226f6ec3","deepnote_cell_type":"markdown"},"source":["After looking at some of the articles data, we found some information that can be extracted as the model features, such as, the ***day*** and ***time*** the article is published.\n","- for each article, we find the 3 words pattern of the ***day*** and ***time*** pattern\n","- the ***day*** data is converted into numerical representation where Sun, Mon, Tue,... is mapped into 0, 1, 2,...\n","- the ***time*** data in converted into minutes"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"39840c788634434fa6ee4ca8d4a7d33f","deepnote_cell_type":"code"},"outputs":[],"source":["def get_day_time(datetime_value):\n","    \"\"\"Extract the day of the week and time from a datetime string.\"\"\"\n","    day = {'Sun': 0, 'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat':6}\n","    value = {}\n","\n","    # Define a regular expression pattern to capture the day of the week and time\n","    pattern = r'^(\\w+), (\\d+ \\w+ \\d{4}) (\\d+:\\d+)'\n","\n","    # Use re.search to find a match in the datetime value\n","    match = re.search(pattern, datetime_value)\n","\n","    if match:\n","        value['Day'] = day[match.group(1)]\n","        if value['Day'] == 0 or value['Day'] == 6:\n","            value['Day'] = 1\n","        else:\n","            value['Day'] = 0\n","        time = match.group(3).split(':')\n","        value['Time'] = (int(time[0]) *60 + int(time[1])) / 1440.0\n","        # date = match.group(2)\n","    else:\n","        value['Day'] = 0\n","        value['Time'] = 0\n","        # date = np.nan\n","    \n","    return value"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"cc3170b076ab46b59d67652d002b5b16","deepnote_cell_type":"code"},"outputs":[],"source":["def chunk(iterable, chunk_size):\n","    \"\"\"Generates lists of `chunk_size` elements from `iterable`.\n","    \n","    \n","    >>> list(chunk((2, 3, 5, 7), 3))\n","    [[2, 3, 5], [7]]\n","    >>> list(chunk((2, 3, 5, 7), 2))\n","    [[2, 3], [5, 7]]\n","    \"\"\"\n","    iterable = iter(iterable)\n","    while True:\n","        chunk = []\n","        try:\n","            for _ in range(chunk_size):\n","                chunk.append(next(iterable))\n","            yield chunk\n","        except StopIteration:\n","            if chunk:\n","                yield chunk\n","            break"]},{"cell_type":"markdown","metadata":{"cell_id":"876dba674aee4a3682b433a5671a8e32","deepnote_cell_type":"markdown"},"source":["- Using ***BeautifulSoup(data, 'html.parser')*** we were able to extract specific part of the HTML based article content. in this case, We split the article ***Title*** and the ***Content*** into individual part and use it as feature of our model by getting the content of **time** and **section** tag of the article\n","- We also <u>count the number of \"video\"</u> provided in the text article by extracting the **iframe** section from the HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"107cda4c57b74a5fad7d1f121a68a47b","deepnote_cell_type":"code"},"outputs":[],"source":["from icecream import ic\n","def preprocess(data):\n","    soup = BeautifulSoup(data, 'html.parser')\n","\n","    title = soup.find('h1', class_='title').text\n","    day = \"Mon\"\n","    day_time = {'Day': 0, 'Time': 0}\n","    try:\n","        day = soup.find('time').get('datetime')\n","        day_time = get_day_time(day)\n","    except:\n","        pass\n","\n","    content_element = soup.find('section', class_='article-content')\n","    content = content_element.text\n","    # links = soup.find_all('a')\n","    # try:\n","    #     links = len(links)\n","    # except:\n","    #     links = 0\n","    vid = soup.find_all('iframe')\n","    arr = [title, content, len(content.split()), len(vid), day_time['Day'], day_time['Time']]\n","\n","    for i in range(2):\n","        arr[i] = preprocessor(arr[i])\n","\n","    return arr"]},{"cell_type":"markdown","metadata":{"cell_id":"acab222b8e1744a29f179824805dcffd","deepnote_cell_type":"markdown"},"source":["- Punctuation marks might be useful in certain NLP contexts but here we remove all punctuation marks for simplicity. One exception is the emoticon characters such as \":)\" since they are certainly useful for sentiment analysis. Furthermore, we convert all text to lowercase since it doesn't matter if reviews are in upper or lower case.\n","- Then the result of the filtered article content is **stemmed** and **stop-words** are removed by using nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2a8071ad452149f6b2ed3af192d70a18","deepnote_cell_type":"code"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package names to /home/jojo/nltk_data...\n","[nltk_data]   Package names is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/jojo/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package movie_reviews to\n","[nltk_data]     /home/jojo/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/jojo/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /home/jojo/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/jojo/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["def preprocessor(text):\n","    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n","    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n","    emoticons = re.findall(r, text)\n","    text = re.sub(r, '', text)\n","\n","    # convert to lowercase and append all emoticons behind (with space in between)\n","    # replace('-','') removes nose of emoticons\n","    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n","    return text\n","def tokenizer(text):\n","    return re.split('\\s+', text.strip())\n","\n","from nltk.stem.porter import PorterStemmer\n","import nltk\n","from nltk.corpus import stopwords\n","\n","def tokenizer_stem(text):\n","    porter = PorterStemmer()\n","    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n","\n","\n","nltk.download([\n","    \"names\",\n","    \"stopwords\",\n","    \"movie_reviews\",\n","    \"averaged_perceptron_tagger\",\n","    \"vader_lexicon\",\n","    \"punkt\",\n","])\n","stop = stopwords.words('english')\n","def tokenizer_stem_nostop(text):\n","    porter = PorterStemmer()\n","    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n","            if w not in stop and re.match('[a-zA-Z]+', w)]"]},{"cell_type":"markdown","metadata":{"cell_id":"c7e244bebec34acbb6941a6397d66632","deepnote_cell_type":"markdown"},"source":["- Listing all the feature for column. \n","NB: should be noted that althought we did extract the **time** information of the article, we didn't end up using it to train our model"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2aa598e36f944a248da6ad2c25fa6815","deepnote_cell_type":"code"},"outputs":[],"source":["feat = ['Title', 'Content', 'WordCount', 'VideoCount', 'Day', 'Time']"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b3a9ebcb031b4561a33e813d592e4bbc","deepnote_cell_type":"code"},"outputs":[],"source":["train_preprocess = train['Page content'].apply(preprocess)\n","test_preprocess = test['Page content'].apply(preprocess)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"bd81ec744e764af88a946fdecf8f5efd","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["['Title', 'Content', 'WordCount', 'VideoCount', 'Day', 'Time']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["feat"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5fa93bd1fc414592b67d1bc6de9aefcd","deepnote_cell_type":"code"},"outputs":[],"source":["train_preprocess_df = pd.DataFrame(train_preprocess.to_list(), columns=feat)\n","test_preprocess_df = pd.DataFrame(test_preprocess.to_list(), columns=feat)\n","\n","train_preprocess_df = pd.concat([train_preprocess_df, train[['Popularity']]], axis=1)\n","test_preprocess_df = pd.concat([test_preprocess_df], axis=1)\n","\n","# scale the data\n","from sklearn.preprocessing import MinMaxScaler\n","sc = MinMaxScaler()\n","train_preprocess_df[['WordCount', 'VideoCount']] = sc.fit_transform(train_preprocess_df[['WordCount', 'VideoCount']])\n","test_preprocess_df[['WordCount', 'VideoCount']] = sc.transform(test_preprocess_df[['WordCount', 'VideoCount']])"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"85e8a3ac5c2f4f7ebec2a1d80a0473f0","deepnote_cell_type":"code"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Content</th>\n","      <th>WordCount</th>\n","      <th>VideoCount</th>\n","      <th>Day</th>\n","      <th>Time</th>\n","      <th>Popularity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>nasa s grand challenge stop asteroids from des...</td>\n","      <td>There may be killer asteroids headed for Eart...</td>\n","      <td>0.069157</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.627778</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>google s new open source patent pledge we won ...</td>\n","      <td>Google took a stand of sorts against patent-l...</td>\n","      <td>0.035626</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.736111</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ballin 2014 nfl draft picks get to choose thei...</td>\n","      <td>You've spend countless hours training to be a...</td>\n","      <td>0.135355</td>\n","      <td>0.274725</td>\n","      <td>0</td>\n","      <td>0.802083</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>cameraperson fails deliver slapstick laughs</td>\n","      <td>Tired of the same old sports fails and ne...</td>\n","      <td>0.032298</td>\n","      <td>0.230769</td>\n","      <td>0</td>\n","      <td>0.101389</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nfl star helps young fan prove friendship with...</td>\n","      <td>At 6-foot-5 and 298 pounds, All-Pro NFL star ...</td>\n","      <td>0.166913</td>\n","      <td>0.010989</td>\n","      <td>0</td>\n","      <td>0.146528</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27638</th>\n","      <td>chief of usaid doesn t know who created cuban ...</td>\n","      <td>The chief of the U.S. Agency for Internationa...</td>\n","      <td>0.035380</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.684722</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>27639</th>\n","      <td>photo of samsung s rumored virtual reality hea...</td>\n","      <td>Back in May, reports surfaced claiming that S...</td>\n","      <td>0.037106</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.043750</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>27640</th>\n","      <td>14 dogs that frankly cannot take the heat</td>\n","      <td>There's nothing more helpless than the middle...</td>\n","      <td>0.018984</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.520833</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>27641</th>\n","      <td>yahoo earnings beat estimates but core problem...</td>\n","      <td>Yahoo's profits in the first quarter beat Wal...</td>\n","      <td>0.051036</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.867361</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>27642</th>\n","      <td>the winners of our curiocity contest tour aust...</td>\n","      <td>Originality. Creativity. Ingenuity. In addit...</td>\n","      <td>0.056213</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.765278</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>27643 rows × 7 columns</p>\n","</div>"],"text/plain":["                                                   Title   \n","0      nasa s grand challenge stop asteroids from des...  \\\n","1      google s new open source patent pledge we won ...   \n","2      ballin 2014 nfl draft picks get to choose thei...   \n","3           cameraperson fails deliver slapstick laughs    \n","4      nfl star helps young fan prove friendship with...   \n","...                                                  ...   \n","27638  chief of usaid doesn t know who created cuban ...   \n","27639  photo of samsung s rumored virtual reality hea...   \n","27640         14 dogs that frankly cannot take the heat    \n","27641  yahoo earnings beat estimates but core problem...   \n","27642  the winners of our curiocity contest tour aust...   \n","\n","                                                 Content  WordCount   \n","0       There may be killer asteroids headed for Eart...   0.069157  \\\n","1       Google took a stand of sorts against patent-l...   0.035626   \n","2       You've spend countless hours training to be a...   0.135355   \n","3           Tired of the same old sports fails and ne...   0.032298   \n","4       At 6-foot-5 and 298 pounds, All-Pro NFL star ...   0.166913   \n","...                                                  ...        ...   \n","27638   The chief of the U.S. Agency for Internationa...   0.035380   \n","27639   Back in May, reports surfaced claiming that S...   0.037106   \n","27640   There's nothing more helpless than the middle...   0.018984   \n","27641   Yahoo's profits in the first quarter beat Wal...   0.051036   \n","27642    Originality. Creativity. Ingenuity. In addit...   0.056213   \n","\n","       VideoCount  Day      Time  Popularity  \n","0        0.000000    0  0.627778          -1  \n","1        0.000000    0  0.736111           1  \n","2        0.274725    0  0.802083           1  \n","3        0.230769    0  0.101389          -1  \n","4        0.010989    0  0.146528          -1  \n","...           ...  ...       ...         ...  \n","27638    0.000000    0  0.684722          -1  \n","27639    0.000000    0  0.043750          -1  \n","27640    0.000000    0  0.520833          -1  \n","27641    0.000000    0  0.867361          -1  \n","27642    0.000000    0  0.765278           1  \n","\n","[27643 rows x 7 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_preprocess_df"]},{"cell_type":"markdown","metadata":{"cell_id":"0c26ffef8f5b40eb85996e34fcca6ae4","deepnote_cell_type":"markdown"},"source":["- Save the new version of preprocessed dataset as csv file to avoid repetitive preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d1753296fd8641869e8c3d318313eac9","deepnote_cell_type":"code"},"outputs":[],"source":["train_preprocess_df.to_csv('data/train_preprocess.csv', index=False)\n","test_preprocess_df.to_csv('data/test_preprocess.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b47909a4db3b4eb48267699396fd3d28","deepnote_cell_type":"code"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Content</th>\n","      <th>WordCount</th>\n","      <th>VideoCount</th>\n","      <th>Day</th>\n","      <th>Time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>soccer star gets twitter death threats after t...</td>\n","      <td>Note to humanity: One Direction fandom ai...</td>\n","      <td>0.062993</td>\n","      <td>0.076923</td>\n","      <td>0</td>\n","      <td>0.824306</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>google glass gets an accessory store</td>\n","      <td>Shortly after announcing a hardware upgrade f...</td>\n","      <td>0.015533</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.392361</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ouya gaming console already sold out on amazon</td>\n","      <td>Well, that was quick. Just hours after going ...</td>\n","      <td>0.018245</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.537500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>between two ferns mocks oscar nominees</td>\n","      <td>Between Two Ferns: Oscar Buzz Edition Part 1...</td>\n","      <td>0.016889</td>\n","      <td>0.043956</td>\n","      <td>0</td>\n","      <td>0.145833</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>american sniper trailer looks like eastwood m...</td>\n","      <td>Ever since The Hurt Locker it seems like ...</td>\n","      <td>0.025025</td>\n","      <td>0.010989</td>\n","      <td>0</td>\n","      <td>0.065278</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11842</th>\n","      <td>check out twitter co founders beautiful new of...</td>\n","      <td>Some of the creators of Twitter have landed ...</td>\n","      <td>0.013190</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","      <td>0.875000</td>\n","    </tr>\n","    <tr>\n","      <th>11843</th>\n","      <td>fish out of water is an adorably addictive io...</td>\n","      <td>Full disclosure: Jetpack Joyride is still on...</td>\n","      <td>0.067678</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.793750</td>\n","    </tr>\n","    <tr>\n","      <th>11844</th>\n","      <td>if music services influenced grammys this is w...</td>\n","      <td>LOS ANGELES — Nominees for the 56th Grammy Aw...</td>\n","      <td>0.117973</td>\n","      <td>0.241758</td>\n","      <td>0</td>\n","      <td>0.984722</td>\n","    </tr>\n","    <tr>\n","      <th>11845</th>\n","      <td>google s nexus 7 comes to europe australia and...</td>\n","      <td>The latest version of Google's Nexus 7 has be...</td>\n","      <td>0.014053</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0.359028</td>\n","    </tr>\n","    <tr>\n","      <th>11846</th>\n","      <td>even samsung can t screw up the galaxy tab s</td>\n","      <td>The Samsung Galaxy Tab S is so much like ...</td>\n","      <td>0.215483</td>\n","      <td>0.010989</td>\n","      <td>0</td>\n","      <td>0.541667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11847 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                   Title   \n","0      soccer star gets twitter death threats after t...  \\\n","1                  google glass gets an accessory store    \n","2        ouya gaming console already sold out on amazon    \n","3                between two ferns mocks oscar nominees    \n","4       american sniper trailer looks like eastwood m...   \n","...                                                  ...   \n","11842  check out twitter co founders beautiful new of...   \n","11843   fish out of water is an adorably addictive io...   \n","11844  if music services influenced grammys this is w...   \n","11845  google s nexus 7 comes to europe australia and...   \n","11846      even samsung can t screw up the galaxy tab s    \n","\n","                                                 Content  WordCount   \n","0           Note to humanity: One Direction fandom ai...   0.062993  \\\n","1       Shortly after announcing a hardware upgrade f...   0.015533   \n","2       Well, that was quick. Just hours after going ...   0.018245   \n","3        Between Two Ferns: Oscar Buzz Edition Part 1...   0.016889   \n","4           Ever since The Hurt Locker it seems like ...   0.025025   \n","...                                                  ...        ...   \n","11842    Some of the creators of Twitter have landed ...   0.013190   \n","11843    Full disclosure: Jetpack Joyride is still on...   0.067678   \n","11844   LOS ANGELES — Nominees for the 56th Grammy Aw...   0.117973   \n","11845   The latest version of Google's Nexus 7 has be...   0.014053   \n","11846       The Samsung Galaxy Tab S is so much like ...   0.215483   \n","\n","       VideoCount  Day      Time  \n","0        0.076923    0  0.824306  \n","1        0.000000    0  0.392361  \n","2        0.000000    0  0.537500  \n","3        0.043956    0  0.145833  \n","4        0.010989    0  0.065278  \n","...           ...  ...       ...  \n","11842    0.000000    1  0.875000  \n","11843    0.000000    0  0.793750  \n","11844    0.241758    0  0.984722  \n","11845    0.000000    0  0.359028  \n","11846    0.010989    0  0.541667  \n","\n","[11847 rows x 6 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_preprocess_df\n","test_preprocess_df"]},{"cell_type":"markdown","metadata":{"cell_id":"3dbf2eddc19b48e6a5912532ca632ca4","deepnote_cell_type":"markdown"},"source":["### Model Building\n","\n","The features we choose to use are\n","- HashingVectorizer(Title)\n","- HashingVectorizer(Content)\n","- WordCount\n","- VideoCount\n","- Day (isWeekend)\n","\n","We try two different methods:\n","- Logistic Regression \n","- Random Forest (What we used)\n","\n","##### Logistic Regression with SGD classifier\n","The dataset size also influence how we choose the model, after the data is represented as feature vectors, it is too large to fit in a model, we choose to use __Out-of-Core learning__ (which is similar to stochastic gradient descent, which divide it into minibatch)\n","Hashing Vectorizer is useful because it does not need to know all the vocabulary space in advance. The result for this method isn't that good\n","\n","\n","\n","##### Random Forest Classifier (depth = 4)\n","We use Random Forest because it combines multiple decision trees, which is good for Ensemble Learning. It can help improve overfitting and improve generalization. It also good for ignoring outliers and noisy data. And it is easier to implement. We used depth = 4 because we have checked that when we increase the depth it does not increase the accuracy and the validation accuracy also drops. So, we figured out that it might be overfitting the training data when depth > 4.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f70dd9055fba45ec9e085c9b7db0a878","deepnote_cell_type":"code"},"outputs":[],"source":["def df_chunk(df, n):\n","    \"\"\"Yield successive n-sized chunks from df.\"\"\"\n","    for i in range(0, len(df), n):\n","        yield df.iloc[i:i + n, :]\n","def transpose(l):\n","    return list(map(list, zip(*l)))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"18c608ac3887491f964acf612ca35a91","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2000/27643] 0.48590895261885314\n","[4000/27643] 0.49758488962644953\n","[6000/27643] 0.4998939995759983\n","[8000/27643] 0.5225890112704917\n","[10000/27643] 0.5344087105554995\n","[12000/27643] 0.5274909896756284\n","[14000/27643] 0.5325165353574559\n","[16000/27643] 0.5521838786196303\n","[18000/27643] 0.511588271301175\n","[20000/27643] 0.48708135900796445\n","[22000/27643] 0.5246742123463115\n","[24000/27643] 0.5373565977055633\n","[26000/27643] 0.5085060340241361\n","[28000/27643] 0.5203692593524414\n"]}],"source":["from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","\n","X_train_title, y_train = train_preprocess_df['Title'].values, train_preprocess_df['Popularity'].values\n","X_train_content = train_preprocess_df['Content'].values\n","X_train_2 = train_preprocess_df[['WordCount', 'VideoCount', 'Day']].values\n","\n","def train_and_validate_model(X_train, X_train_2, X_train_opt=None, y_train=y_train, batch_size=1000, n_features=2**10, max_iter=20, tol=1e-3):\n","    hashvec = HashingVectorizer(n_features=n_features, preprocessor=None, tokenizer=tokenizer_stem_nostop)\n","\n","    # loss='log' gives logistic regression\n","    clf = SGDClassifier(loss='log_loss', max_iter=max_iter, tol=tol)\n","    \n","    stream_X = chunk(X_train, batch_size)\n","    stream_X2 = chunk(X_train_2, batch_size)\n","    stream_y = chunk(y_train, batch_size)\n","    stream_x_opt = chunk(X_train_opt, batch_size) if X_train_opt is not None else None\n","\n","    classes = np.array([-1, 1])\n","    train_auc, val_auc = [], []\n","\n","    # we use one batch for training and another for validation in each iteration\n","    iters = int((len(X_train) + batch_size - 1) / (batch_size * 2))\n","\n","    for i in range(iters):\n","        stream_x_chunk = next(stream_X)\n","        stream_x2_chunk = next(stream_X2)\n","        stream_y_chunk = next(stream_y)\n","        stream_x_opt_chunk = next(stream_x_opt) if stream_x_opt is not None else None\n","        if stream_x_chunk is None:\n","            break\n","        stream_x_chunk = hashvec.transform(stream_x_chunk)\n","        if stream_x_opt_chunk:\n","            stream_x_opt_chunk = hashvec.transform(stream_x_opt_chunk)\n","            stream_train_combined = np.concatenate((stream_x_chunk.toarray(), stream_x_opt_chunk.toarray(), stream_x2_chunk), axis=1)\n","        else:\n","            stream_train_combined = np.concatenate((stream_x_chunk.toarray(), stream_x2_chunk), axis=1)\n","\n","        clf.partial_fit(stream_train_combined, stream_y_chunk, classes=classes)\n","        train_auc.append(roc_auc_score(stream_y_chunk, clf.predict_proba(stream_train_combined)[:, 1]))\n","\n","        # validate\n","        X_val, y_val = next(stream_X), next(stream_y)\n","        X_val_2 = next(stream_X2)\n","        X_val_opt = next(stream_x_opt) if stream_x_opt is not None else None\n","\n","\n","        X_val = hashvec.transform(X_val)\n","        if X_val_opt:\n","            X_val_opt = hashvec.transform(X_val_opt)\n","            X_val_combined = np.concatenate((X_val.toarray(), X_val_opt.toarray(), X_val_2), axis=1)\n","        else:\n","            X_val_combined = np.concatenate((X_val.toarray(), X_val_2), axis=1)\n","        score = roc_auc_score(y_val, clf.predict_proba(X_val_combined)[:, 1])\n","        val_auc.append(score)\n","        print('[{}/{}] {}'.format((i + 1) * (batch_size * 2), len(X_train), score))\n","    \n","    return train_auc, val_auc, clf\n","\n","train_auc, val_auc, clf = train_and_validate_model(X_train_title, X_train_2, X_train_content, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1a667d1e286d48f1a4e12a49b5760f1f","deepnote_cell_type":"code"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","hashvec = HashingVectorizer(n_features=2**10, preprocessor=None, tokenizer=tokenizer_stem_nostop)\n","X_train_combined_ = np.concatenate((hashvec.transform(X_train_title).toarray(), hashvec.transform(X_train_content).toarray(), X_train_2), axis=1)\n","y_train = train_preprocess_df['Popularity'].values\n","\n","scores = cross_val_score(estimator=RandomForestClassifier(max_depth=4, random_state=0),\n","                         X=X_train_combined_, y=y_train,\n","                         cv=10, scoring='roc_auc')"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"39e1277a26d74295b53485b36e5ef1f6","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["array([0.54818031, 0.56624458, 0.55319279, 0.53970175, 0.54342643,\n","       0.55868777, 0.54109946, 0.55318232, 0.55257119, 0.55697848])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["scores"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7252970319e64c4e9ab618c0f6b0ad32","deepnote_cell_type":"code"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=4, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=4, random_state=0)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(max_depth=4, random_state=0)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["forest = RandomForestClassifier(max_depth=4, random_state=0)\n","forest.fit(X_train_combined_, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f9ca23bfb5294c75b8f16951de506f6c","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.49190449 0.49327882 0.48912734 ... 0.48529063 0.48017796 0.48947631]\n"]}],"source":["X_train_title, y_train = train_preprocess_df['Title'].values, train_preprocess_df['Popularity'].values\n","X_train_content = train_preprocess_df['Content'].values\n","X_train_2 = train_preprocess_df[['WordCount', 'VideoCount', 'Day']].values\n","\n","X_test_title = test_preprocess_df['Title'].values\n","X_test_content = test_preprocess_df['Content'].values\n","X_test_2 = test_preprocess_df[['WordCount', 'VideoCount', 'Day']].values\n","X_test_combined = np.concatenate((hashvec.transform(X_test_title).toarray(), hashvec.transform(X_test_content).toarray(), X_test_2), axis=1)\n","y_pred = forest.predict_proba(X_test_combined)\n","y_pred = y_pred[:,1]\n","print(y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["### Save to CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8d4fc995b2db4a5dbec15c1dea7524c0","deepnote_cell_type":"code"},"outputs":[],"source":["# write to csv\n","df = pd.DataFrame({'Id': test['Id'], 'Popularity': y_pred})\n","df.to_csv('data/submission_new.csv', index=False)"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["### Conclusions\n","\n","Some ideas to take away is that by just taking the body content, preprocess and tokenize it. The accuracy was lower and that is why we changed our approach into getting some other features such as video count, word count, etc. This is because we have noticed that this thing might be the feature needed and is related to a news' popularity. The results using random forest classifier are better than the logistic regression. \n","\n","In this competition, we also learn some pitfalls, we have tried to look into VADER polarity scoring, which can be described as the __emotion intensity__ score, it ranges from -4 to 4, where -4 is the most negative and +4 is the most positive. But it seems like this feature does not be able to determine that the article is popular or not. (Sometimes some article might have a negative score, but it is more popular, because human might like an article that sparks more emotion.)\n","\n","The other pitfalls that we have noticed is that we didn't check our training and validation loss. We noticed too late that it might have helped us in getting better result during the competiton. "]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"2a091f00c8be440cb60a225ea1a554fa","kernelspec":{"display_name":"sleep-linear","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
